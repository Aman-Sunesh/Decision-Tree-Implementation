{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08a15a4e-a121-4e26-9573-c2386e883d84",
   "metadata": {},
   "source": [
    "# Mini Project 3 - Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3f92e4-a8c1-479e-81ac-0c9faf938a2f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, we implement a Decision Tree for classification. A decision tree is a predictive model that maps observations (features) to conclusions (target labels) by recursively splitting the data into more homogeneous subsets. For our datasets, which include the Iris (3 classes) and Spambase (binary classification) datasets, the splits are binary and are determined by maximizing the **information gain** based on the reduction in **entropy**.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "- **Entropy:** A measure of the impurity or uncertainty in the target variable.  \n",
    "  $$ H(Y) = -\\sum_{i} p_i \\log_2(p_i) $$\n",
    "\n",
    "- **Information Gain:** The reduction in entropy achieved by splitting a node.  \n",
    "  $$ IG = H(\\text{parent}) - \\left(\\frac{N_{left}}{N_{parent}} H(\\text{left}) + \\frac{N_{right}}{N_{parent}} H(\\text{right})\\right) $$\n",
    "\n",
    "- **Early Stopping with n_min:** Instead of growing the tree until pure leaves, we stop splitting if a node contains fewer than a threshold number of samples n_min, defined as a percentage of the training set.\n",
    "\n",
    "\n",
    "For the Iris dataset, we will try n\\_min ∈ {5, 10, 15, 20} and use 10-fold cross-validation. For Spambase, we will use n\\_min ∈ {5, 10, 15, 20, 25}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc4e108-4041-4a4a-8885-2978f61b8eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a372772-43c1-4b23-af97-f34820773594",
   "metadata": {},
   "source": [
    "## Node Representation\n",
    "\n",
    "We define a `Node` class to represent a decision point in our tree. Each node has the following attributes:\n",
    "\n",
    "- **feature_name**: (or index) The feature used for splitting at that node.\n",
    "- **threshold**: The numeric threshold used for the split (e.g., \"petal_length < 2.5\").\n",
    "- **left** and **right**: The left and right child nodes, respectively.  \n",
    "  (The left child corresponds to samples where the feature value is less than or equal to the threshold.)\n",
    "- **is_leaf**: A Boolean flag indicating whether the node is a leaf.\n",
    "- **value**: If the node is a leaf, this stores the predicted class label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df59e5b4-86bc-4bbc-b42c-413eb4c0738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for each node in the decision tree\n",
    "class Node():\n",
    "    def __init__(self, feature_index = None, threshold = None, left = None, right = None, is_leaf = False, value = None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.is_leaf = is_leaf\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621a0ff-e08b-413e-9676-84e0589113c7",
   "metadata": {},
   "source": [
    "## Decision Tree Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7da1f5ff-3977-42ea-bc97-d62e092b0924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the Decision Tree model\n",
    "class DecisionTree():\n",
    "    def __init__(self, n_min = 5, training_size=None):\n",
    "        self.n_min = n_min # Minimum number of samples required to split a node\n",
    "        self.training_size = training_size\n",
    "        self.root = None\n",
    "\n",
    "    def compute_entropy(self, y):\n",
    "        counts = np.bincount(y.astype(int)) # Count occurrences of each class\n",
    "        probabilities = counts / len(y)     # Convert to probabilities\n",
    "        entropy = 0\n",
    "\n",
    "        for p in probabilities:\n",
    "            if p > 0:\n",
    "                entropy += -p * math.log2(p)\n",
    "\n",
    "        return entropy\n",
    "\n",
    "    def compute_information_gain(self, feature_column, y, threshold):\n",
    "        original_entropy = self.compute_entropy(y)\n",
    "\n",
    "        # Split data into left and right based on threshold\n",
    "        left_mask = (feature_column <= threshold)\n",
    "        right_mask = (feature_column > threshold)\n",
    "\n",
    "        y_left = y[left_mask]\n",
    "        y_right = y[right_mask]\n",
    "\n",
    "        n = len(y)\n",
    "        n_left = len(y_left)\n",
    "        n_right = len(y_right)\n",
    "\n",
    "        if n_left == 0 or n_right == 0:\n",
    "            return 0  # If one child is empty, no gain.\n",
    "\n",
    "        # Compute \"weighted\" entropy after split\n",
    "        child_entropy = ((n_left / n) * self.compute_entropy(y_left)) + ((n_right / n) * self.compute_entropy(y_right))\n",
    "        \n",
    "        return original_entropy - child_entropy\n",
    "\n",
    "    def choose_best_split(self, X, y):\n",
    "        best_gain = -1 # Initalizing gain to a large -ve value\n",
    "        best_feature_index = None\n",
    "        best_threshold = None\n",
    "\n",
    "        n_samples, n_features = X.shape  \n",
    "\n",
    "        for feature_index in range(n_features):\n",
    "            values = np.unique(X[:, feature_index])  # Get unique feature values\n",
    "            values.sort() # Sort values to create threshold candidates\n",
    "\n",
    "            if len(values) < 2:\n",
    "                continue # Skip if there is no possible split\n",
    "            \n",
    "            for i in range(len(values) - 1):\n",
    "                threshold = (values[i] + values[i + 1]) / 2  # The midpoints of consecutive unique values are taken as thresholds\n",
    "                gain = self.compute_information_gain(X[:, feature_index], y, threshold)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature_index, best_threshold\n",
    "\n",
    "    def create_tree(self, X, y, min_samples_leaf):\n",
    "        # Stop if all samples belong to one class or too few samples to split\n",
    "        if len(np.unique(y)) == 1 or len(y) < min_samples_leaf:\n",
    "            counts = np.bincount(y.astype(int))   # Count class occurrences\n",
    "            majority_label = np.argmax(counts)    # Assign majority class label\n",
    "            return Node(value = majority_label, is_leaf = True)   # Create leaf node\n",
    "\n",
    "        best_feature, best_threshold = self.choose_best_split(X, y)   # Find best split\n",
    "\n",
    "        if best_feature is None:\n",
    "            counts = np.bincount(y.astype(int))\n",
    "            majority_label = np.argmax(counts)\n",
    "            return Node(value = majority_label, is_leaf = True)   # Create leaf node\n",
    "\n",
    "        node = Node(feature_index = best_feature, threshold = best_threshold, is_leaf = False)   # Create decision node\n",
    "\n",
    "        # Split data into left and right based on best feature and threshold\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        X_left, y_left = X[left_indices], y[left_indices]\n",
    "        X_right, y_right = X[right_indices], y[right_indices]\n",
    "\n",
    "        if len(y_left) == 0 or len(y_right) == 0:\n",
    "            counts = np.bincount(y.astype(int))\n",
    "            majority_label = np.argmax(counts)\n",
    "            return Node(value = majority_label, is_leaf = True)   # Create leaf node\n",
    "\n",
    "        # Recursively create left and right child nodes\n",
    "        node.left = self.create_tree(X_left, y_left, min_samples_leaf )\n",
    "        node.right = self.create_tree(X_right, y_right, min_samples_leaf )\n",
    "\n",
    "        return node\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        min_samples_leaf = math.ceil((self.n_min / 100) * len(X))\n",
    "        self.root = self.create_tree(X, y, min_samples_leaf)\n",
    "\n",
    "    def predict_sample(self, x, node):\n",
    "        if node.is_leaf:\n",
    "            return node.value   # Return class label if leaf node\n",
    "        else:\n",
    "            if x[node.feature_index] <= node.threshold:\n",
    "                return self.predict_sample(x, node.left)   # Go left if condition is met\n",
    "            else:\n",
    "                return self.predict_sample(x, node.right)  # Otherwise, go right\n",
    "\n",
    "    def predict(self, X):\n",
    "        n = X.shape[0]\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            sample = X[i]\n",
    "            prediction = self.predict_sample(sample, self.root)\n",
    "            predictions.append(prediction)\n",
    "        \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be217e79-390f-427f-a34d-3830f08113f6",
   "metadata": {},
   "source": [
    "## Model Evaluation on the Iris Dataset\n",
    "\n",
    "We now perform 10-fold cross-validation to test our decision tree for various values of n_min. We record the mean accuracy and standard deviation for each n_min value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef51b14b-14c1-4b2a-8b19-9dcd0a2d25db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris_df = pd.read_csv(\"iris.csv\", names=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"])\n",
    "with pd.option_context('future.no_silent_downcasting', True):\n",
    "    iris_df= iris_df.replace({'Iris-setosa':0, 'Iris-versicolor': 1, 'Iris-virginica': 2}).infer_objects()\n",
    "\n",
    "X = iris_df.iloc[:, :-1].values  \n",
    "y = iris_df.iloc[:, -1].values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15867818-7782-45b7-84de-8d786dcf26b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold cross-validation\n",
    "kf = KFold(n_splits = 10, shuffle=True, random_state=42)\n",
    "n_min_vals = {5, 10, 15, 20}\n",
    "results = []\n",
    "\n",
    "for n_min in n_min_vals:\n",
    "    accuracy_vals = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Splitting the dataset into train set and test set\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Train Decision Tree with current n_min\n",
    "        tree = DecisionTree(n_min = n_min)\n",
    "        tree.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = tree.predict(X_test)\n",
    "\n",
    "        # Compute accuracy\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        accuracy_vals.append(acc)\n",
    "\n",
    "    # Compute mean and standard deviation of accuracy\n",
    "    avg_accuracy = np.mean(accuracy_vals)\n",
    "    std_accuracy = np.std(accuracy_vals)\n",
    "\n",
    "    results.append({\"n_min\": n_min, \"Mean Accuracy\": avg_accuracy, \"Std Dev\": std_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7048f0c-cf4e-42e6-896c-f1d15764d197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_min</th>\n",
       "      <th>Mean Accuracy</th>\n",
       "      <th>Std Dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.049889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.049889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.049889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.049889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_min  Mean Accuracy   Std Dev\n",
       "0     10       0.946667  0.049889\n",
       "1     20       0.946667  0.049889\n",
       "2      5       0.946667  0.049889\n",
       "3     15       0.946667  0.049889"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d38d474-3015-4298-8247-76726c4d9391",
   "metadata": {},
   "source": [
    "## Model Evaluation on the Spambase Dataset\n",
    "\n",
    "Repeat a similar evaluation using the Spambase dataset. Use n_min values from {5, 10, 15, 20, 25} and perform 10-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4453d939-14c2-40bc-b3ad-524bd532f98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spambase dataset\n",
    "spambase_df = pd.read_csv(\"spambase.csv\")\n",
    "\n",
    "X = spambase_df.iloc[:, :-1].values  \n",
    "y = spambase_df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dac8e135-e2b3-4cfd-bb71-1dd5a3eb1300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold cross-validation\n",
    "kf = KFold(n_splits = 10, shuffle=True, random_state=42)\n",
    "n_min_vals = {5, 10, 15, 20, 25}\n",
    "results = []\n",
    "\n",
    "for n_min in n_min_vals:\n",
    "    accuracy_vals = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Splitting the dataset into train set and test set\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Train Decision Tree with current n_min\n",
    "        tree = DecisionTree(n_min = n_min)\n",
    "        tree.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = tree.predict(X_test)\n",
    "\n",
    "        # Compute accuracy\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        accuracy_vals.append(acc)\n",
    "\n",
    "    # Compute mean and standard deviation of accuracy\n",
    "    avg_accuracy = np.mean(accuracy_vals)\n",
    "    std_accuracy = np.std(accuracy_vals)\n",
    "\n",
    "    results.append({\"n_min\": n_min, \"Mean Accuracy\": avg_accuracy, \"Std Dev\": std_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "070498a9-03a1-42b5-9c09-4aa0b736a92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_min</th>\n",
       "      <th>Mean Accuracy</th>\n",
       "      <th>Std Dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.858913</td>\n",
       "      <td>0.020450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.901522</td>\n",
       "      <td>0.019687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>0.827609</td>\n",
       "      <td>0.016529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.888696</td>\n",
       "      <td>0.014544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>0.868478</td>\n",
       "      <td>0.023216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_min  Mean Accuracy   Std Dev\n",
       "0     20       0.858913  0.020450\n",
       "1      5       0.901522  0.019687\n",
       "2     25       0.827609  0.016529\n",
       "3     10       0.888696  0.014544\n",
       "4     15       0.868478  0.023216"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82720553-f4ed-4107-9950-47b93ebc8a96",
   "metadata": {},
   "source": [
    "## Conclusion & Discussion\n",
    "\n",
    "- The decision tree was built recursively by choosing the best split based on information gain.\n",
    "- Early stopping was applied using the n_min threshold to prevent overfitting.\n",
    "- We evaluated our model using 10-fold cross-validation on both the Iris and Spambase datasets.\n",
    "- The results (mean accuracy and standard deviation) are summarized in the tables above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4236b9d3-31ff-4654-9775-28c9e6d748b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
